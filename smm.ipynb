{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 参数设置\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.num_classes = 10\n",
    "        self.epochs = 50\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = 64\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.whether_pretrained = False\n",
    "        self.whether_save = False\n",
    "        self.save_name = None\n",
    "        self.seed = 42\n",
    "        self.reprogram_model = None\n",
    "        self.model = \"resnet18\"\n",
    "        self.seed = 0 \n",
    "        self.patch_size = 8\n",
    "        self.attribute_channels = 3 \n",
    "        self.attribute_layers = 5\n",
    "        self.fraction = 1.0 \n",
    "        self.local_import_model = False\n",
    "        self.mapping_method = \"rlm\"\n",
    "        self.imgsize = 224\n",
    "        self.attr_gamma = 0.1\n",
    "        self.attr_lr = 0.01\n",
    "\n",
    "# 训练ResNet模型\n",
    "def train_resnet_model(train_dataset, \n",
    "                       test_dataset, \n",
    "                       num_classes, \n",
    "                       log_dir=\"runs/exp\",\n",
    "                      agrs=None,\n",
    "                       save_name=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    args = Args()\n",
    "    # Load ResNet18\n",
    "    model = models.resnet18(pretrained=args.whether_pretrained)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    \n",
    "    #Tensorboard Write in \n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    # Training loops\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images,labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{args.epochs}\", unit=\"batch\"):\n",
    "        # for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0   \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        test_accuracy = 100 * test_correct / test_total\n",
    "        # Log the loss and accuracy\n",
    "        writer.add_scalar('Loss/test', test_loss/test_total, epoch)\n",
    "        writer.add_scalar('Accuracy/test', test_accuracy, epoch)\n",
    "        # Log the training loss and accuracy    \n",
    "        writer.add_scalar('Loss/train', running_loss/total, epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{args.epochs}], Loss: {running_loss/total:.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Loss: {test_loss/test_total:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    if args.whether_save and save_name!= None:\n",
    "        torch.save(model.state_dict(), save_name)\n",
    "        print(f\"Model saved as {save_name}\")\n",
    "    return model        \n",
    "\n",
    "from smm import InstancewiseVisualPrompt\n",
    "\n",
    "# model reprogramming\n",
    "from functools import partial\n",
    "from smm import generate_label_mapping_by_frequency, label_mapping_base\n",
    "def reprogram_model(train_dataset,test_dataset,base_model):\n",
    "    args = Args()\n",
    "    device = args.device\n",
    "    if args.model == \"ViT_B32\":\n",
    "        args.imgsize = 384\n",
    "    else:\n",
    "        args.imgsize = 224\n",
    "    model = base_model\n",
    "    # load the dataset \n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    \n",
    "    # if device is None:\n",
    "    #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # import the base model\n",
    "    if base_model == \"resnet18\":\n",
    "        # 如果是导入本地模型\n",
    "        if args.local_import_model:\n",
    "            base_model = models.resnet18(pretrained=False)\n",
    "            base_model.fc = nn.Linear(model.fc.in_features,30)\n",
    "            base_model.load_state_dict(torch.load('resnet_quickdraw_30class.pth'))\n",
    "            base_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # else:\n",
    "        #     # 导入 imagenet 参数\n",
    "        #     from torchvision.models import resnet18, ResNet18_Weights\n",
    "        #     base_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1).to(device)\n",
    "        #     base_model = models.resnet18(weight_path)\n",
    "        #     base_model.fc = nn.Linear(base_model.fc.in_features, args.num_classes)\n",
    "    # initialize the visual prompting model\n",
    "    base_model.requires_grad_(False)\n",
    "    base_model.eval()\n",
    "    vp = visual_prompt = InstancewiseVisualPrompt(args.imgsize, args.attribute_layers, args.patch_size, args.attribute_channels).to(device)\n",
    "    optimizer = torch.optim.Adam([{'params': visual_prompt.program, 'lr': args.lr}])\n",
    "    optimizer_att = torch.optim.Adam([{'params': visual_prompt.priority.parameters(), 'lr': args.attr_lr}])\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                     milestones=[int(0.5 * args.epochs), int(0.72 * args.epochs)],\n",
    "                                                     gamma=0.1)\n",
    "    scheduler_att = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                    milestones=[int(0.5 * args.epochs), int(0.72 * args.epochs)],\n",
    "                                                    gamma=args.attr_gamma)\n",
    "    class_names = [str(i) for i in range(args.num_classes)]\n",
    "    if args.mapping_method == 'rlm':\n",
    "        mapping_sequence = torch.randperm(1000)[:len(class_names)]\n",
    "        label_mapping = partial(label_mapping_base, mapping_sequence=mapping_sequence)\n",
    "    elif args.mapping_method == 'flm':\n",
    "        mapping_sequence = generate_label_mapping_by_frequency(visual_prompt, base_model, train_loader)\n",
    "        label_mapping = partial(label_mapping_base, mapping_sequence=mapping_sequence)\n",
    "    # Train\n",
    "    train_accs = []\n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "\n",
    "\n",
    "    best_acc = 0.\n",
    "    tr_acc = 0\n",
    "    scaler = GradScaler()\n",
    "    print(\"start training\")\n",
    "\n",
    "\n",
    "    log_dir_smm = f\"./logs/smm\"\n",
    "    writer = SummaryWriter(log_dir=log_dir_smm)\n",
    "\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    import torch.nn.functional as F\n",
    "    import time\n",
    "    for epoch in range(args.epochs):\n",
    "        start_time = time.time()\n",
    "        if args.mapping_method == 'ilm':\n",
    "            mapping_sequence = generate_label_mapping_by_frequency(visual_prompt, base_model, train_loader)\n",
    "            label_mapping = partial(label_mapping_base, mapping_sequence=mapping_sequence)\n",
    "        visual_prompt.train()\n",
    "        total_num = 0\n",
    "        true_num = 0\n",
    "        loss_sum = 0\n",
    "        print(f\"Epoch {epoch+1}/{args.epochs} training\")\n",
    "        pbar = tqdm(train_loader, total=len(train_loader),\n",
    "                    desc=f\"Epo {epoch}\", ncols=100)\n",
    "        for x, y in pbar:\n",
    "            if x.get_device() == -1:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "            pbar.set_description_str(f\"Epo {epoch}\", refresh=True)\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_att.zero_grad()\n",
    "            with autocast():\n",
    "                fx = label_mapping(base_model(visual_prompt(x)))\n",
    "                loss = F.cross_entropy(fx, y, reduction='mean')\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.step(optimizer_att)\n",
    "            scaler.update()\n",
    "            total_num += y.size(0)\n",
    "            true_num += torch.argmax(fx, 1).eq(y).float().sum().item()\n",
    "            loss_sum += loss.item() * fx.size(0)\n",
    "            pbar.set_postfix_str(f\"Acc {100 * true_num / total_num:.2f}%\")\n",
    "        epoch_time = time.time() - start_time\n",
    "        scheduler.step()\n",
    "        scheduler_att.step()\n",
    "        \n",
    "        train_accs.append(true_num / total_num)\n",
    "        tr_acc = true_num / total_num\n",
    "        train_losses.append(loss_sum / total_num)\n",
    "        print(\"train/acc\", true_num / total_num, epoch)\n",
    "        print(\"train/loss\", loss_sum / total_num, epoch)\n",
    "        writer.add_scalar(\"train/acc\", true_num / total_num, epoch)\n",
    "        writer.add_scalar(\"train/loss\", loss_sum / total_num, epoch)\n",
    "        writer.add_scalar(\"train/time\", epoch_time, epoch)\n",
    "\n",
    "\n",
    "        # Test\n",
    "        visual_prompt.eval()\n",
    "        total_num = 0\n",
    "        true_num = 0\n",
    "        loss_sum = 0\n",
    "        pbar = tqdm(test_loader, total=len(test_loader), desc=f\"Epo {epoch} Testing\", ncols=100)\n",
    "        ys = []\n",
    "        for x, y in pbar:\n",
    "            if x.get_device() == -1:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "            ys.append(y)\n",
    "            with torch.no_grad():\n",
    "                fx0 = base_model(visual_prompt(x))\n",
    "                fx = label_mapping(fx0)\n",
    "                loss = F.cross_entropy(fx, y, reduction='mean')\n",
    "            loss_sum += loss.item() * fx.size(0)\n",
    "            total_num += y.size(0)\n",
    "            true_num += torch.argmax(fx, 1).eq(y).float().sum().item()\n",
    "            acc = true_num / total_num\n",
    "            pbar.set_postfix_str(f\"Acc {100 * acc:.2f}%\")\n",
    "        print(\"test/acc\", acc, epoch)\n",
    "        print(\"test/loss\", loss_sum / total_num, epoch)\n",
    "        writer.add_scalar(\"test/acc\", acc, epoch)\n",
    "        writer.add_scalar(\"test/loss\", loss_sum / total_num, epoch)\n",
    "\n",
    "\n",
    "\n",
    "        # save data \n",
    "        train_acc = tr_acc\n",
    "        train_loss = loss_sum / total_num\n",
    "        train_accs.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # ====== 写入日志文件 ======\n",
    "        writer.flush()\n",
    "\n",
    "        # Save CKPT\n",
    "        state_dict = {\n",
    "            \"visual_prompt_dict\": visual_prompt.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_acc\": best_acc,\n",
    "            \"mapping_sequence\": mapping_sequence,\n",
    "        }\n",
    "        if acc > best_acc:\n",
    "        #     best_acc = acc\n",
    "        #     state_dict['best_acc'] = best_acc\n",
    "        #     torch.save(state_dict, os.path.join(save_path, 'best.pth'))\n",
    "        # torch.save(state_dict, os.path.join(save_path, 'ckpt.pth'))\n",
    "            print(f\"Epoch {epoch} best accuracy: {best_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
